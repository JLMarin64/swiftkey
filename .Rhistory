Means <- apply(data, 1, mean)
Means <- apply(data, 1, mean)
ActualMean <- mean(Means)
TheorySD<- ((1/L)) * (1/sqrt(n)))
TheorySD<- ((1/L) * (1/sqrt(n)))
SD <- sd(Means)
TheoryVararice <- TheorySD^2
Variance <- var(Means)
meanCompare <- data.frame(Mean.Title = c("Sample Mean", "Theoretical Mean"), Mean.Values = c(Mean,TheoryMean))
meanCompare <- data.frame(Mean.Title = c("Sample Mean", "Theoretical Mean"), c(Mean,TheoryMean))
meanCompare <- data.frame(Mean.Title = c("Sample Mean", "Theoretical Mean"), Mean.Values = c(Mean,TheoryMean))
meanCompare <- data.frame(Mean.Title = c("Sample Mean", "Theoretical Mean"))
View(meanCompare)
Mean <- mean(Means)
meanCompare <- data.frame(Mean.Title = c("Sample Mean", "Theoretical Mean"), Mean.Values = c(Mean,TheoryMean))
TheoryMean <- 1/L
Means <- apply(data, 1, mean)
Mean <- mean(Means)
TheorySD<- ((1/L) * (1/sqrt(n)))
SD <- sd(Means)
TheoryVararice <- TheorySD^2
Variance <- var(Means)
meanCompare <- data.frame(Mean.Title = c("Sample Mean", "Theoretical Mean"), Mean.Values = c(Mean,TheoryMean))
View(meanCompare)
The Theoretical mean is `r meanCompare$Mean.values[1]`
meanCompare$Mean.Values[0]
meanCompare$Mean.Values[1]
meanCompare$Mean.Values[2]
View(meanCompare)
The Theoretical mean is `r meanCompare$Mean.values[2]` and the sample mean is `r meanCompare$Mean.values[1]`. The difference between the two means is
The Theoretical mean is `r meanCompare$Mean.values[2]` and the sample mean is `r meanCompare$Mean.values[1]`. The difference between the two means is
meanCompare$Mean.Values[2] - meanCompare$Mean.Values[1]
meanCompare
for (i in 1 :1000) {
data = c(data, mean(rexp(n, L)))
}
Means <- apply(data, 1, mean)
data <- matrix(rexp(n*observations, L), observations)
View(data)
data <- NULL
for (i in 1 :1000) {
data = c(data, mean(rexp(n, L)))
}
head(data)
TheoryMean <- 1/L
Means <- apply(data, 1, mean)
for (i in 1: observations) {
as.dataframe(simulations = cbind(simulations, rexp(n,L)))
}
for (i in 1: observations) {
date.frame(simulations = cbind(simulations, rexp(n,L)))
}
for (i in 1: observations) {
date.frame(simulations = cbind(simulations, rexp(n,L)))
}
simulations <- matrix(rexp(observations * n, rate= L), observations, n)
simMean <- rowMeans(simulations)
data <- rowMeans(simulations)
TheoryMean <- 1/L
Means <- apply(data, 1, mean)
Mean <- mean(data)
TheorySD<- ((1/L) * (1/sqrt(n)))
data <- matrix(rexp(n*observations, L), observations)
Means <- apply(data, 1, mean)
Mean <- mean(Means)
TheorySD<- ((1/L) * (1/sqrt(n)))
SD <- sd(Means)
TheoryVararice <- TheorySD^2
Variance <- var(Means)
meanCompare <- data.frame(Mean.Title = c("Sample Mean", "Theoretical Mean"), Mean.Values = c(Mean,TheoryMean))
meanCompare
Mean
Means
MeansDF <- data.frame(Means)
MeansDF
plot <- ggplot(MeansDF, aes(x=Means))
library(ggplot2)
plot <- ggplot(MeansDF, aes(x=Means))
install.packages("ggplot2")
plot <- ggplot(MeansDF, aes(x=Means))
g <- ggplot(MeansDF, aes(x=Means))
library(ggplot2)
g <- ggplot(MeansDF, aes(x=Means))
plot <- ggplot(MeansDF, aes(x=Means))
plot
plot <- plot + geom_histogram(binwidth = .3, color = "black", fill = "red")
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .3, color = "black", fill = "red")
plot
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red")
plot
plot <- plot + geom_vline(xintercept = TheoryMean, color = "blue", sie = 1, linetype = 2)
plot
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red")
plot <- plot + geom_vline(xintercept = TheoryMean, color = "blue", sie = 2, linetype = 2)
plot <- plot _ geom_vline(xintercept = Mean, color = "pink", size = 2, linetype =3)
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red")
plot <- plot + geom_vline(xintercept = TheoryMean, color = "blue", sie = 2, linetype = 2)
plot <- plot + geom_vline(xintercept = Mean, color = "pink", size = 2, linetype =3)
plot
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red")
plot <- plot + geom_vline(xintercept = TheoryMean, color = "blue", sie = 2, linetype = 2)
plot <- plot + geom_vline(xintercept = Mean, color = "green", size = 2, linetype =3)
plot
In the figure above, the blue dashed line represents the Theoretical mean of `r TheoryMean` and the green squared line reprents the actual mean of `r Mean`.  Both means are very close.
TheoryVariance <- TheorySD^2
Variance <- var(Means)
TheoryVariance
Variance
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red"), aes(y=..density))
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red", aes(y=..density))
plot
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red"
plot
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red")
plot
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red")+
stat_function(fun=dnorm, args=list(mean=therosampmean, sd=sd(Means)), color = "blue", size = 1) +
stat_density(geom = "line", color = "green", size =1)  +
labs(x="mean of 40 simulated exponential sample", y= "density",
title="Density of Simulated Exponential Samples Means")
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red")+
stat_function(fun=dnorm, args=list(mean=TheoryMean, sd=sd(Means)), color = "blue", size = 1) +
stat_density(geom = "line", color = "green", size =1)  +
labs(x="mean of 40 simulated exponential sample", y= "density",
title="Density of Simulated Exponential Samples Means")
plot
plot2 <- ggplot(data, aes(x=Means))
plot2 <- ggplot(data, aes(x=Means))
plot2 <- ggplot(MeansDF, aes(x=Means))
plot2 <- plot2 + geom_histogram(binwidth = .2, color="black", fill="blue" , aes(y=..density..))+
stat_function(fun=dnorm, args=list(mean=TheoryMean, sd=sd(Means)),
color="red", size =1) +
stat_density(geom = "line", color = "green", size =1)  +
labs(x="mean of 40 simulated exponential sample", y= "density",
title="Density of Simulated Exponential Samples Means")
plot2
plot2 <- ggplot(MeansDF, aes(x=Means))
plot2 <- plot2 + geom_histogram(binwidth = .2, color="black", fill="red" , aes(y=..density..))+
stat_function(fun=dnorm, args=list(mean=TheoryMean, sd=sd(Means)),
color="yellow", size =1) +
stat_density(geom = "line", color = "blue", size =1)  +
labs(x="mean of 40 simulated exponential sample", y= "density",
title="Density of Simulated Exponential Samples Means")
plot2
plot <- ggplot(MeansDF, aes(x=Means))
plot <- plot + geom_histogram(binwidth = .1, color = "black", fill = "red")
plot <- plot + geom_vline(xintercept = TheoryMean, color = "blue", sie = 2, linetype = 2)
plot <- plot + geom_vline(xintercept = Mean, color = "green", size = 2, linetype =3)
plot <- plot + labs(x= "Means")
plot
qqnorm(Means)
qqline(Means, col = "red")
install.packages("pdflatex")
sudo apt-get install r-cran-e1071
library("caret", lib.loc="~/R/win-library/3.4")
install.packages(pkgs = C:\Users\Marin Family\Downloads\e1071_1.6-8, repos = NULL)
library(e1071)
knitr::opts_chunk$set(echo = TRUE)
modelFit <- train(type ~., data = training, method = "glm")
library(caret)   #
library(kernlab) #contains the data for spam data set
##We want 75% of the data in the training set
inTrain <- createDataPartition(y=spam$type), p=.75, list = FALSE)
inTrain <- createDataPartition(y=spam$type), p=.75, list = FALSE)
data(spam)
inTrain <- createDataPartition(y=spam$type), p=.75, list = FALSE)
library(caret)
library(kernlab) #contains the data for spam data set
inTrain <- createDataPartition(y=spam$type), p=.75, list = FALSE)
inTrain <- createDataPartition(y=spam$type, p=.75, list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
modelFit <- train(type ~., data = training, method = "glm")
set.seed(1235)
#We are predicting type
modelFit <- train(type ~., data = training, method = "glm")
modelFit
modelFit$method
modelFit$finalModel
modelFit
modelFit$results
modelFit$bestTune
modelFit$pred
modelFit$dots
install.packages("ISLR")
library(ISLR)
library(ggplot2)
data(wage)
library(ISLR)
library(ggplot2)
data(wage)
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=.7, list = FALSE)
training <- wage[inTrain,]
training <- Wage[inTrain,]
test <- Wage[-inTrain,]
dim(training)
dim(training); dim(testing)
featurePlot(x=training[,c("age", "education", "jobclass")]), y = training$wage, plot = "pairs")
featurePlot(x=training[,c("age", "education", "jobclass")], y = training$wage, plot = "pairs")
qplot(age, wage, colour=jobclass, data = training)
qq <- qplot(age, wage, colour= education, data = training)
qq + geom_smooth(method='lm', formula = y~x)
install.packages("Hmisc")
install.packages("Hmisc")
knitr::opts_chunk$set(echo = TRUE)
library(Hmisc)
cutWage <- cut2(training$wage, g3)
cutWage <- cut2(training$wage, g=3)
table(cutWage)
p1 <- qplot(cutWage, age, data = training, fill = cutWage, geom=c("boxplot"))
p1
p1 <- qplot(cutWage, age, data = training, fill = cutWage, geom=c("boxplot", "jitter"))
p1
t1 <- table(cutWage, training$jobclass)
t1
prop.table(t1, 1)
prop.table(t1, 1)
qplot(wage, colour=education, data = training, geom="density")
hist(training$capitalAve, main = "", xlab="ave. capital run length")
hist(training$capitalAve)
set.seed(1235)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=.75, list = FALSE)
library(caret)   #
library(kernlab) #contains the data for spam data set
set.seed(1235)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=.75, list = FALSE)
#partitioning the data from spam that is in training and not in training
training <- spam[inTrain,]
testing <- spam[-inTrain,]
hist(training$capitalAve)
mean(training$capitalAve)
sd(training$capitalAve)
trainCapAveS < (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
mean(trainCapAveS)
sd(trainCapAveS)
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
library(caret)
data("faithful")
data(faithful)
set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting, p = .5, list = FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue")
lm1 = lm(eruptions ~ waiting, data = trainFaith)
summary(lm1)
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue")
lines(trainFaith$waiting, lm1$fitted, lwd = 3)
coef(lm1[1] + coef(lm1)[2]*80)
coef(lm1)[1] + coef(lm1)[2]*80)
coef(lm1)[1] + coef(lm1)[2]*80
newdata <- data.frame(waiting = 80)
predict(lm1, newdata)
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
sqrt(sum((predict(lm1, newdata = testFaith)-testFaith$eruptions)^2))
pred1 <- predict(lm1, newdata= testFaith, interval= "prediction")
ord <- order(testFaith$waiting)
plot(testFaith, testFaith$eruptions, pch= 19, col = "blue")
matlines(testFaith$waiting[ord, pred1[ord,], type="l"],,col=c(1,2,2), lty = c(1,1,1), lwd=3)
matlines(testFaith$waiting[ord], pred1[ord,], type="l"],,col=c(1,2,2), lty = c(1,1,1), lwd=3)
matlines(testFaith$waiting[ord], pred1[ord,], type="l",,col=c(1,2,2), lty = c(1,1,1), lwd=3)
matlines(testFaith$waiting[ord], pred1[ord,], type="l",,col=c(1,2,2), lty = c(1,1,1), lwd=3)
modFit <- train(eruptions ~ waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
Wage <- subset(Wage, select = -c(logwage))
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=.7, list = FALSE)
inTrain <- createDataPartition(y=Wage$wage, p=.7, list = FALSE)
training <- Wage[inTrain,]
test <- Wage[-inTrain,]
dim(training)
dim(test)
featurePlot(x=training[,c("age", "education", "jobclass")], y = training$wage, plot= "pairs")
qplot(age, wage, data = training)
qplot(age, wage, colour = jobclass, data = training)
qplot(age, wage, colour = education, data = training)
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
plot(finMod, 1, pch= 19, cex = .5, col="#000000010")
plot(finMod, 1, pch= 19, cex = .5, col="#00000010")
qplot(finMod$fitted, finMod$residuals, colour = race, data = training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(rattle)
install.packages("rattle")
data(iris)
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p= .7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[inTrain,]
dim(training); dim(testing)
qplot(Petal.Width, Sepal.Width, colour = Species, data = training)
library(caret)
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFi
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex=.8)
install.packages("rattle")
library(rattle)
library(rattle)
library(rattle)
library(rattle)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
library(rattle)
library(RGtk2)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit$finalModel)
installed.packages(rattle)
library(rattle)
install.packages(rattle)
install.packages("rattle")
library(rattle)
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p= .7, list = FALSE)
data(iris)
library(ggplot2)
library(caret)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species, p= .7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[inTrain,]
dim(training); dim(testing)
qplot(Petal.Width, Sepal.Width, colour = Species, data = training)
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
mem.limits(size = 8000)
mem.limits(size = 6000)
memory.limit(size = 8000)
memory.limit(size = 80000)
#Load Libraries
library(caret)
library(randomForest)
#Set Working Direcotry
setwd("C:/Users/Marin Family/Documents/R/Machine Learning Assignment")
#Set Seed
set.seed(1234)
#Load Downloaded Train and Test Files from Working Directory
training <- read.csv(file="pml-training.csv", na.strings=c("", "NA", "NULL"))
testing <- read.csv(file="pml-testing.csv", na.strings=c("", "NA", "NULL"))
#Remove NA columns and indentifier columns
training <- training[, colSums(is.na(training))==0]
dim(training)
#After review, the first 7 columns as they do not seem relevant to the outcome.
training <- training[,-c(1:7)]
testing <- testing[, -c(1:7)]
dim(training)
dim(testing)
#Create my test set
inTrain <- createDataPartition(y=training$classe, p=.7, list = FALSE)
trainset <- training[inTrain,]
#Create My Test Data
mytestset <- training[-inTrain,]
dim(trainset)
dim(mytestset)
#Load Coursera Test Data
testset <- read.csv("pml-testing.csv")
dim(testset)
modelfit <- train(classe ~., data = trainset, method = "rf", prox = TRUE)
modelfit$finalmodel
modelfit$finalmodel
modelfit <- train(classe ~., data = trainset, method = "rf")
modelfit$finalmodel
fitControl <- trainControl(method = "cv",
number = 10,
allowParallel = TRUE)
modelfit <- train(classe ~., data = trainset, method = "rf", trControl = fitControl)
library(caret)
library(randomForest)
library(parallel)
library(doParallel)
install.packages("doParallel")
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
#Load Libraries
library(caret)
library(randomForest)
library(parallel)
library(doParallel)
#Set Working Direcotry
setwd("C:/Users/Marin Family/Documents/R/Machine Learning Assignment")
#Set Seed
set.seed(1234)
#Load Downloaded Train and Test Files from Working Directory
training <- read.csv(file="pml-training.csv", na.strings=c("", "NA", "NULL"))
testing <- read.csv(file="pml-testing.csv", na.strings=c("", "NA", "NULL"))
#Remove NA columns and indentifier columns
training <- training[, colSums(is.na(training))==0]
dim(training)
#After review, the first 7 columns as they do not seem relevant to the outcome.
training <- training[,-c(1:7)]
testing <- testing[, -c(1:7)]
dim(training)
dim(testing)
#Create my test set
inTrain <- createDataPartition(y=training$classe, p=.7, list = FALSE)
trainset <- training[inTrain,]
#Create My Test Data
mytestset <- training[-inTrain,]
dim(trainset)
dim(mytestset)
#Load Coursera Test Data
testset <- read.csv("pml-testing.csv")
dim(testset)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv",
number = 10,
allowParallel = TRUE)
modelfit <- train(classe ~., data = trainset, method = "rf", trControl = fitControl)
modelfit$finalmodel
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv",
number = 3,
allowParallel = TRUE)
modelfit <- train(classe ~., data = trainset, method = "rf", trControl = fitControl)
modelfit$finalmodel
corpus <- Corpus(VectorSource(c(blogs_sub, news_sub, twitter_sub)), readerControl=list(reader=readPlain,language="en"))
library(tm)
library(RWeka)
install.packages("ggplot")
corpus <- Corpus(VectorSource(c(blogs_sub, news_sub, twitter_sub)), readerControl=list(reader=readPlain,language="en"))
blogs <- readLines("./en_us.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
library(tm)
library(RWeka)
library(tm)
library(RWeka)
install.packages('rJava')
library(tm)
library(RWeka)
library(rJava)
library(rJava)
library(RWeka)
install.packages("libname")
library(RWeka)
install.packages("RWeka")
library(RWeka)
library(tm)
library(rJava)
install.packees('pkgname')
install.packages('pkgname')
library(RWeka)
library(rJava)
library(RWeka)
setwd("C:/Users/Marin Family/Desktop/New folder/Coursera-SwiftKey/final/en_US")
getwd()
setwd("C:/Users/Marin Family/Desktop/New folder/Coursera-SwiftKey/final/en_US")
setwd("C:/Users/Marin Family/Desktop/New folder/Coursera-SwiftKey/final/en_US")
getwd()
blogs <- readLines("./en_us.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
blogs <- readLines("./en_us.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
